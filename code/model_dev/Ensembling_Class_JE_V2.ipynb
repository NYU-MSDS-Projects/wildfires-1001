{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import packages\n",
    "import pandas as pd  # provides interface for interacting with tabular data\n",
    "import geopandas as gpd  # combines the capabilities of pandas and shapely for geospatial operations\n",
    "import rtree  # supports geospatial join\n",
    "import os\n",
    "import fnmatch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "import pickle\n",
    "from shapely.ops import nearest_points\n",
    "from datetime import datetime as dt, date\n",
    "sys.path.append('/Users/jackepstein/Documents/GitHub/wildfires-1001/code/functions/')\n",
    "from modeling_functions import *\n",
    "data_dir = '/Users/jackepstein/Documents/GitHub/wildfires-1001/data'\n",
    "code_dir = '/Users/jackepstein/Documents/GitHub/wildfires-1001/code'\n",
    "model_dir = '/Users/jackepstein/Documents/GitHub/wildfires-1001/models'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report, multilabel_confusion_matrix\n",
    "from sklearn.metrics import auc, roc_curve, roc_auc_score\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC, LinearSVC\n",
    "from sklearn.preprocessing import scale, label_binarize\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pull in main data frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pull in the target data frame and weather dictionary \n",
    "#make sure to change the pkl file name if needed\n",
    "target_dict = {}\n",
    "target_df = gpd.GeoDataFrame()\n",
    "for i in np.arange(1, 3):\n",
    "    target_dict[i] = pd.read_pickle(os.path.join(data_dir, f'clean_data/target_df_final_1123_newtargets_{i}.pkl')) \n",
    "    target_df = target_df.append(target_dict[i])\n",
    "\n",
    "\n",
    "weather_dict_path = os.path.join(data_dir, 'clean_data/ERA_weather-data/ERA_rename_dictionary.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load the naming dictionary\n",
    "with open(weather_dict_path, 'rb') as handle:\n",
    "    rename_dict = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#rename the columns based on this dictionary\n",
    "target_df.rename(columns = rename_dict, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create lists of columns to drop and what our targets are\n",
    "non_mod_cols = ['GRID_ID','month_id','MONTH','COUNTYFP','COUNTY_AREA', 'NAME','GRID_AREA','COUNTY_ARE','month_id_old_x','month_id_old_y',\n",
    "                'geometry','Fire_area','total_fire_days','hist_p_time_1y','total_fire_days','hist_p_time_1y', \n",
    "                'hist_p_time_1m', 'month_id_old', 'YEAR','Index','index']\n",
    "bad_features = ['hist_p_time_1m', 'total_fire_days', 'hist_p_time_1y','month_id_old']\n",
    "Y_cols = ['Y_bin', 'Y_fire_count', 'Y_fire_area_prop', 'Y_fire_class_size','Y_bin_new_fire_month',\n",
    "          'Y_max_new_fire_size_month','Y_count_new_fires_month']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert floats from 64 to 32 for model\n",
    "for col in target_df.columns:\n",
    "    if target_df[col].dtypes == 'float64':\n",
    "        target_df[col] = target_df[col].astype(np.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pull in Models and Feature Lists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pull in models\n",
    "\n",
    "#list of models\n",
    "model_list = ['LR_15PCA_1990_2015.pkl', 'LR_30entropy_1990_2015.pkl', 'linSVC_25PCA_1990_2015.pkl', \n",
    "              'LR_15PCA_1990_2005.pkl', 'LR_20gini_1990_2005.pkl', 'linSVC_15PCA_1990_2005.pkl', \n",
    "              'linSVC_30gini_1990_2005.pkl', 'linSVC_35entropy_1990_2015.pkl']\n",
    "\n",
    "\n",
    "#get all paths for loading\n",
    "model_path_list = []\n",
    "for m in model_list:\n",
    "    mod_path = os.path.join(model_dir, m)\n",
    "    model_path_list.append(mod_path)\n",
    "\n",
    "#load the models into a dictionary with the file as the key and the model as the value\n",
    "models = {}\n",
    "for m in range(len(model_list)):\n",
    "    with open(model_path_list[m], 'rb') as handle:\n",
    "        models[model_list[m]] = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pull in feature lists\n",
    "\n",
    "#w ill need 30e\n",
    "feat_list = ['RF_entropy_top30_features.pkl', 'RF_gini_top20_features_1990_2005.pkl', \n",
    "             'RF_gini_top30_features_1990_2005.pkl', 'RF_entropy_top35_features.pkl']\n",
    "\n",
    "#get paths for loading\n",
    "feat_path_list = []\n",
    "for f in feat_list:\n",
    "    feat_path = os.path.join(model_dir,'feature_lists',f)\n",
    "    feat_path_list.append(feat_path)\n",
    "\n",
    "features = {}\n",
    "for f in range(len(feat_list)):\n",
    "    with open(feat_path_list[f], 'rb') as handle:\n",
    "        features[feat_list[f]] = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Split training and testing data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This is the split where using the 1990-2005 on initial test, 2006-2015 on 2nd stage and 2016-2019 on final testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#generate phase1 data set\n",
    "#pre 2006\n",
    "phase1_data = target_df[target_df['YEAR']<2006]\n",
    "X_phase1 = phase1_data.drop('YEAR', axis = 1)\n",
    "#drop columns not used for modeling - dont drop Ys here\n",
    "for y in Y_cols + non_mod_cols + bad_features:\n",
    "    try:\n",
    "        X_phase1.drop(y, inplace = True, axis =1)\n",
    "    except:\n",
    "        pass\n",
    "#set up target variable\n",
    "Y_ph1_cl = phase1_data[['Y_bin_new_fire_month']]\n",
    "Y_ph1_cl_size = phase1_data[['Y_max_new_fire_size_month']]\n",
    "Y_ph1_cl_arr = Y_ph1_cl.to_numpy().ravel()\n",
    "Y_ph1_cl_size_arr = Y_ph1_cl_size.to_numpy().ravel()\n",
    "\n",
    "#generate phase2 data set - same logic as above\n",
    "phase2_data = target_df[(target_df['YEAR']>=2006)&(target_df['YEAR']<2016)]\n",
    "X_phase2 = phase2_data.drop('YEAR', axis = 1)\n",
    "for y in Y_cols + non_mod_cols + bad_features:\n",
    "    try:\n",
    "        X_phase2.drop(y, inplace = True, axis =1)\n",
    "    except:\n",
    "        pass\n",
    "Y_ph2_cl = phase2_data[['Y_bin_new_fire_month']]\n",
    "Y_ph2_cl_size = phase2_data[['Y_max_new_fire_size_month']]\n",
    "Y_ph2_cl_arr = Y_ph2_cl.to_numpy().ravel()\n",
    "Y_ph2_cl_size_arr = Y_ph2_cl_size.to_numpy().ravel()\n",
    "\n",
    "#generate phase3 (test) data set\n",
    "phase3_data = target_df[target_df['YEAR']>=2016]\n",
    "X_phase3 = phase3_data.drop('YEAR', axis = 1)\n",
    "for y in Y_cols + non_mod_cols + bad_features:\n",
    "    try:\n",
    "        X_phase3.drop(y, inplace = True, axis =1)\n",
    "    except:\n",
    "        pass\n",
    "Y_ph3_cl = phase3_data[['Y_bin_new_fire_month']]\n",
    "Y_ph3_cl_size = phase3_data[['Y_max_new_fire_size_month']]\n",
    "Y_ph3_cl_arr = Y_ph3_cl.to_numpy().ravel()\n",
    "Y_ph3_cl_size_arr = Y_ph3_cl_size.to_numpy().ravel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#scale all data sets\n",
    "X_phase1_scaled = pd.DataFrame(scale(X_phase1), columns = X_phase1.columns, index=X_phase1.index)\n",
    "X_phase2_scaled = pd.DataFrame(scale(X_phase2), columns = X_phase2.columns, index=X_phase2.index)\n",
    "X_phase3_scaled = pd.DataFrame(scale(X_phase3), columns = X_phase3.columns, index=X_phase3.index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Perform PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#copy base df and re-drop non modeling columns\n",
    "X_features = target_df.copy()\n",
    "for y in Y_cols + non_mod_cols + bad_features:\n",
    "    try:\n",
    "        X_features.drop(y, inplace = True, axis =1)\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "#scale features    \n",
    "X_features_scaled = scale(X_features)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7fa4c1c71bb0>]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#fit the scaled dataset\n",
    "pca_x = PCA().fit(X_features_scaled)\n",
    "\n",
    "#plot explained variance ratio\n",
    "plt.plot(pca_x.explained_variance_ratio_[0:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca_df = pd.DataFrame(pca_x.components_[0:20,].transpose(), columns = [\"PC{}\".format(i+1) for i in range(20)], \\\n",
    "                      index = X_features.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#write for loop to make list \"nth_principle_component\"\n",
    "princ_comp_list = []\n",
    "for name in range(1,173):\n",
    "    pr = str(name)+'th_pc'\n",
    "    princ_comp_list.append(pr)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#do split on 3 phases -- still need to do 3 phase split as well\n",
    "X_features_pca = pd.DataFrame(pca_x.transform(X_features_scaled), columns = princ_comp_list)\n",
    "X_features_pca['YEAR'] = target_df['YEAR']\n",
    "\n",
    "#split phases on year\n",
    "X_ph1_pca = X_features_pca[X_features_pca['YEAR']< 2006]\n",
    "X_ph2_pca = X_features_pca[(X_features_pca['YEAR']>=2006)&(X_features_pca['YEAR']<2016)]\n",
    "X_ph3_pca = X_features_pca[X_features_pca['YEAR']>=2016]\n",
    "\n",
    "\n",
    "Y_ph1_pca = target_df[target_df['YEAR']<2006]['Y_bin_new_fire_month']\n",
    "Y_ph1_size_pca = target_df[target_df['YEAR']<2006]['Y_max_new_fire_size_month']\n",
    "\n",
    "Y_ph2_pca = target_df[(target_df['YEAR']>=2006)&(target_df['YEAR']<2016)]['Y_bin_new_fire_month']\n",
    "Y_ph2_size_pca = target_df[(target_df['YEAR']>=2006)&(target_df['YEAR']<2016)]['Y_max_new_fire_size_month']\n",
    "\n",
    "Y_ph3_pca = target_df[target_df['YEAR']>=2016]['Y_bin_new_fire_month']\n",
    "Y_ph3_size_pca = target_df[target_df['YEAR']>=2016]['Y_max_new_fire_size_month']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pick model and Get Trained Predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ASPECTS TO TEST\n",
    "- LR vs SVM\n",
    "- REGULARIZATION PARAMETERS\n",
    "- PCA vs DATA\n",
    "- Number of Features/Principle Components\n",
    "- Try one test with Predictions as a feature rather than a filtering device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LinearSVC(C=0.0001, class_weight='balanced', dual=False)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#pick model\n",
    "#picking SVM 'linSVC_15PCA_1990_2005.pkl'\n",
    "#same AUC as LR model, higher recall\n",
    "model = models['linSVC_15PCA_1990_2005.pkl']\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#slim down features\n",
    "X_ph1_15 = X_ph1_pca[X_ph1_pca.columns[0:15]]\n",
    "X_ph2_15 = X_ph2_pca[X_ph2_pca.columns[0:15]]\n",
    "X_ph3_15 = X_ph3_pca[X_ph3_pca.columns[0:15]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#run model on train and test sets to get predictions\n",
    "y_preds_ph2 = model.predict(X_ph2_15)\n",
    "y_preds_ph3 = model.predict(X_ph3_15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#want to append predictions and class size Y to\n",
    "#FULL PCA set (then test the number of features we want)\n",
    "#REGULAR features list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#store predictions and filter -- phase 2\n",
    "\n",
    "#append the these predictions to the dataframe\n",
    "preds2_df = pd.DataFrame(y_preds_ph2, columns=['preds'], index=X_ph2_pca.index)\n",
    "X_ph2_pca_preds = X_ph2_pca.merge(preds2_df, how='outer', left_index=True, right_index=True)\n",
    "\n",
    "#rejoin with the y-size column\n",
    "X_ph2_pca_ysize = X_ph2_pca_preds.merge(Y_ph2_size_pca, how='outer', left_index=True, right_index=True)\n",
    "\n",
    "\n",
    "\n",
    "#store predictions and filter -- phase 3\n",
    "\n",
    "#append the these predictions to the dataframe\n",
    "preds3_df = pd.DataFrame(y_preds_ph3, columns=['preds'], index=X_ph3_pca.index)\n",
    "X_ph3_pca_preds = X_ph3_pca.merge(preds3_df, how='outer', left_index=True, right_index=True)\n",
    "\n",
    "#rejoin with the y-size column\n",
    "X_ph3_pca_ysize = X_ph3_pca_preds.merge(Y_ph3_size_pca, how='outer', left_index=True, right_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#repeat this process for the standard scaled data set\n",
    "\n",
    "#append the these predictions to the dataframe\n",
    "preds2_df2 = pd.DataFrame(y_preds_ph2, columns=['preds'], index=X_phase2_scaled.index)\n",
    "X_ph2_scaled_preds = X_phase2_scaled.merge(preds2_df2, how='outer', left_index=True, right_index=True)\n",
    "\n",
    "#rejoin with the y-size column\n",
    "X_ph2_scaled_ysize = X_ph2_scaled_preds.merge(Y_ph2_cl_size, how='outer', left_index=True, right_index=True)\n",
    "\n",
    "\n",
    "\n",
    "#store predictions and filter -- phase 3\n",
    "\n",
    "#append the these predictions to the dataframe\n",
    "preds3_df2 = pd.DataFrame(y_preds_ph3, columns=['preds'], index=X_phase3_scaled.index)\n",
    "X_ph3_scaled_preds = X_phase3_scaled.merge(preds3_df2, how='outer', left_index=True, right_index=True)\n",
    "\n",
    "#rejoin with the y-size column\n",
    "X_ph3_scaled_ysize = X_ph3_scaled_preds.merge(Y_ph3_cl_size, how='outer', left_index=True, right_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using positive predictions as a filter (rather than a feature)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get test and training sets and targets ready"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#filter for the positive predicted instances\n",
    "X_ph2_pca_cut = X_ph2_pca_ysize.loc[X_ph2_pca_ysize['preds']==1]\n",
    "X_ph3_pca_cut = X_ph3_pca_ysize.loc[X_ph3_pca_ysize['preds']==1]\n",
    "X_ph2_scaled_cut = X_ph2_scaled_ysize.loc[X_ph2_scaled_ysize['preds']==1]\n",
    "X_ph3_scaled_cut = X_ph3_scaled_ysize.loc[X_ph3_scaled_ysize['preds']==1]\n",
    "\n",
    "\n",
    "#drop preds and class_size columns, while saving class size as a new training y\n",
    "Y_ph2_size_pca_cut = X_ph2_pca_cut['Y_max_new_fire_size_month']\n",
    "Y_ph3_size_pca_cut = X_ph3_pca_cut['Y_max_new_fire_size_month']\n",
    "Y_ph2_scaled_cut = X_ph2_scaled_cut['Y_max_new_fire_size_month']\n",
    "Y_ph3_scaled_cut = X_ph3_scaled_cut['Y_max_new_fire_size_month']\n",
    "\n",
    "X_ph2_pca_ready = X_ph2_pca_cut.drop(columns=['preds','Y_max_new_fire_size_month','YEAR'])\n",
    "X_ph3_pca_ready = X_ph3_pca_cut.drop(columns=['preds','Y_max_new_fire_size_month','YEAR'])\n",
    "X_ph2_scaled_ready = X_ph2_scaled_cut.drop(columns=['preds','Y_max_new_fire_size_month'])\n",
    "X_ph3_scaled_ready = X_ph3_scaled_cut.drop(columns=['preds','Y_max_new_fire_size_month'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#regularization hyperparam options\n",
    "cs = [10**i for i in range(-4, 2)] \n",
    "pc_nums = [15, 20, 25, 30]\n",
    "classes = np.array([0,1,2,3])\n",
    "costs = np.array([0, -1, -10 , -50])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  0.   ,  -0.25 ,  -2.5  , -12.5  ],\n",
       "       [ -1.   ,   0.   ,  -0.225,  -0.245],\n",
       "       [-10.   ,  -0.9  ,   0.   ,  -0.2  ],\n",
       "       [-50.   ,  -0.98 ,  -0.8  ,   0.   ]])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#get the cost matrix\n",
    "cost_mat = multi_cost_matrix(classes, costs)\n",
    "cost_mat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test 1: Logistic Regression - PCA\n",
    "- Regularization params\n",
    "- Number of Principle Components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#run LR on all Cs and number of PCs\n",
    "#store all confusion matrices, AUCs, scores and expected values\n",
    "aucs_t1 = {}\n",
    "conf_mats_t1 = {}\n",
    "evs_t1 = {}\n",
    "\n",
    "for i in pc_nums:\n",
    "    #slim down data set to first i PCs\n",
    "    X_ph2_pca_fit = X_ph2_pca_ready[X_ph2_pca_ready.columns[0:i]]\n",
    "    X_ph3_pca_score = X_ph3_pca_ready[X_ph3_pca_ready.columns[0:i]]\n",
    "    \n",
    "    #create nested dictionary \n",
    "    aucs_t1[i] = {}\n",
    "    conf_mats_t1[i] = {}\n",
    "    evs_t1[i] = {}\n",
    "    \n",
    "    for c in cs:\n",
    "        lr = LogisticRegression(C=c, max_iter=1500, class_weight = 'balanced').fit(X_ph2_pca_fit, \n",
    "                                                                                   Y_ph2_size_pca_cut.to_numpy().ravel())\n",
    "\n",
    "        cm = confusion_matrix(Y_ph3_size_pca_cut, lr.predict(X_ph3_pca_score), normalize='true')\n",
    "        conf_mats_t1[i][c] = cm\n",
    "        ev = EV_multi(Y_ph3_size_pca_cut, lr.predict(X_ph3_pca_score), classes, cost_mat)\n",
    "        evs_t1[i][c] = ev\n",
    "        \n",
    "        #store aucs -- will aggregate after taking an auc for each class -- simple mean\n",
    "        y = label_binarize(Y_ph3_size_pca_cut, classes=[0,1,2,3])\n",
    "        n_classes = y.shape[1]\n",
    "        aucs_sub = np.zeros(n_classes)\n",
    "        for n in range(n_classes):\n",
    "            fpr, tpr, thresholds = roc_curve(y[:,n], lr.predict_proba(X_ph3_pca_score)[:,n])\n",
    "            aucs_sub = auc(fpr,tpr)\n",
    "    \n",
    "        aucs_t1[i][c] = np.mean(aucs_sub)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-2.785930161214157"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#get the best expected value from the test\n",
    "#need logic to get parameters for this test\n",
    "max_evs_t1 = []\n",
    "for d in pc_nums:\n",
    "    max_ev = max(evs_t1[d].values())\n",
    "    max_evs_t1.append(max_ev)\n",
    "\n",
    "max(max_evs_t1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test 2: SVM - PCA\n",
    "- Regularization params\n",
    "- Number of Principle Components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#run SVM on all Cs and number of PCs\n",
    "#store all confusion matrices, AUCs, scores and expected values\n",
    "aucs_t2 = {}\n",
    "conf_mats_t2 = {}\n",
    "evs_t2 = {}\n",
    "\n",
    "for i in pc_nums:\n",
    "    #slim down data set to first i PCs\n",
    "    X_ph2_pca_fit = X_ph2_pca_ready[X_ph2_pca_ready.columns[0:i]]\n",
    "    X_ph3_pca_score = X_ph3_pca_ready[X_ph3_pca_ready.columns[0:i]]\n",
    "    \n",
    "    #create nested dictionary \n",
    "    aucs_t2[i] = {}\n",
    "    conf_mats_t2[i] = {}\n",
    "    evs_t2[i] = {}\n",
    "    \n",
    "    for c in cs:\n",
    "        svm = LinearSVC(C=c, class_weight = 'balanced', dual=False).fit(X_ph2_pca_fit, \n",
    "                                                                                   Y_ph2_size_pca_cut.to_numpy().ravel())\n",
    "\n",
    "        cm = confusion_matrix(Y_ph3_size_pca_cut, svm.predict(X_ph3_pca_score), normalize='true')\n",
    "        conf_mats_t2[i][c] = cm\n",
    "        ev = EV_multi(Y_ph3_size_pca_cut, svm.predict(X_ph3_pca_score), classes, cost_mat)\n",
    "        evs_t2[i][c] = ev\n",
    "        \n",
    "        #store aucs -- will aggregate after taking an auc for each class -- simple mean\n",
    "        y = label_binarize(Y_ph3_size_pca_cut, classes=[0,1,2,3])\n",
    "        n_classes = y.shape[1]\n",
    "        aucs_sub = np.zeros(n_classes)\n",
    "        for n in range(n_classes):\n",
    "            fpr, tpr, thresholds = roc_curve(y[:,n], svm.decision_function(X_ph3_pca_score)[:,n])\n",
    "            aucs_sub = auc(fpr,tpr)\n",
    "    \n",
    "        aucs_t2[i][c] = np.mean(aucs_sub)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-6.933039702643486"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#get the best expected value from the test\n",
    "#need logic to get parameters for this test\n",
    "max_evs_t2 = []\n",
    "for d in pc_nums:\n",
    "    max_ev = max(evs_t2[d].values())\n",
    "    max_evs_t2.append(max_ev)\n",
    "\n",
    "max(max_evs_t2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test 3: Logistic Regression - Feature Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Regularization params\n",
    "- Features (can pick from list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "feat_list_keys = list(features.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "#run LR on all Cs and features\n",
    "#store all confusion matrices, AUCs, scores and expected values\n",
    "aucs_t3 = {}\n",
    "conf_mats_t3 = {}\n",
    "evs_t3 = {}\n",
    "\n",
    "for i in feat_list_keys:\n",
    "    #slim down data set our top features\n",
    "    X_ph2_feat_fit = X_ph2_scaled_ready[features[i]]\n",
    "    X_ph3_feat_score = X_ph3_scaled_ready[features[i]]\n",
    "    \n",
    "    #create nested dictionary \n",
    "    aucs_t3[i] = {}\n",
    "    conf_mats_t3[i] = {}\n",
    "    evs_t3[i] = {}\n",
    "    \n",
    "    for c in cs:\n",
    "        lr = LogisticRegression(C=c, max_iter=1500, class_weight = 'balanced').fit(X_ph2_feat_fit, \n",
    "                                                                                   Y_ph2_scaled_cut.to_numpy().ravel())\n",
    "\n",
    "        cm = confusion_matrix(Y_ph3_scaled_cut, lr.predict(X_ph3_feat_score), normalize='true')\n",
    "        conf_mats_t3[i][c] = cm\n",
    "        ev = EV_multi(Y_ph3_scaled_cut, lr.predict(X_ph3_feat_score), classes, cost_mat)\n",
    "        evs_t3[i][c] = ev\n",
    "        \n",
    "        #store aucs -- will aggregate after taking an auc for each class -- simple mean\n",
    "        y = label_binarize(Y_ph3_scaled_cut, classes=[0,1,2,3])\n",
    "        n_classes = y.shape[1]\n",
    "        aucs_sub = np.zeros(n_classes)\n",
    "        for n in range(n_classes):\n",
    "            fpr, tpr, thresholds = roc_curve(y[:,n], lr.predict_proba(X_ph3_feat_score)[:,n])\n",
    "            aucs_sub = auc(fpr,tpr)\n",
    "    \n",
    "        aucs_t3[i][c] = np.mean(aucs_sub)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-2.5454706434438066"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#get the best expected value from the test\n",
    "#need logic to get parameters for this test\n",
    "max_evs_t3 = []\n",
    "for f in feat_list_keys:\n",
    "    max_ev = max(evs_t3[f].values())\n",
    "    max_evs_t3.append(max_ev)\n",
    "\n",
    "max(max_evs_t3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test 4: SVM- Feature Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Regularization params\n",
    "- Features (can pick from list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "#run LR on all Cs and features\n",
    "#store all confusion matrices, AUCs, scores and expected values\n",
    "aucs_t4 = {}\n",
    "conf_mats_t4 = {}\n",
    "evs_t4 = {}\n",
    "\n",
    "for i in feat_list_keys:\n",
    "    #slim down data set our top features\n",
    "    X_ph2_feat_fit = X_ph2_scaled_ready[features[i]]\n",
    "    X_ph3_feat_score = X_ph3_scaled_ready[features[i]]\n",
    "    \n",
    "    #create nested dictionary \n",
    "    aucs_t4[i] = {}\n",
    "    conf_mats_t4[i] = {}\n",
    "    evs_t4[i] = {}\n",
    "    \n",
    "    for c in cs:\n",
    "        svm = LinearSVC(C=c, class_weight = 'balanced', dual=False).fit(X_ph2_feat_fit, \n",
    "                                                                                   Y_ph2_scaled_cut.to_numpy().ravel())\n",
    "\n",
    "        cm = confusion_matrix(Y_ph3_scaled_cut, svm.predict(X_ph3_feat_score), normalize='true')\n",
    "        conf_mats_t4[i][c] = cm\n",
    "        ev = EV_multi(Y_ph3_scaled_cut, svm.predict(X_ph3_feat_score), classes, cost_mat)\n",
    "        evs_t4[i][c] = ev\n",
    "        \n",
    "        #store aucs -- will aggregate after taking an auc for each class -- simple mean\n",
    "        y = label_binarize(Y_ph3_scaled_cut, classes=[0,1,2,3])\n",
    "        n_classes = y.shape[1]\n",
    "        aucs_sub = np.zeros(n_classes)\n",
    "        for n in range(n_classes):\n",
    "            fpr, tpr, thresholds = roc_curve(y[:,n], svm.decision_function(X_ph3_feat_score)[:,n])\n",
    "            aucs_sub = auc(fpr,tpr)\n",
    "    \n",
    "        aucs_t4[i][c] = np.mean(aucs_sub)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-6.176176628020629"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#get the best expected value from the test\n",
    "#need logic to get parameters for this test\n",
    "max_evs_t4 = []\n",
    "for f in feat_list_keys:\n",
    "    max_ev = max(evs_t4[f].values())\n",
    "    max_evs_t4.append(max_ev)\n",
    "\n",
    "max(max_evs_t4)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
