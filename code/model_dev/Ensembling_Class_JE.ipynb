{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Still to do: Print/graph evaluation in digestible format, run on PCA, QA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import packages\n",
    "import pandas as pd  # provides interface for interacting with tabular data\n",
    "import geopandas as gpd  # combines the capabilities of pandas and shapely for geospatial operations\n",
    "import rtree  # supports geospatial join\n",
    "import os\n",
    "import fnmatch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "import pickle\n",
    "from shapely.ops import nearest_points\n",
    "from datetime import datetime as dt, date\n",
    "sys.path.append('/Users/jackepstein/Documents/GitHub/wildfires-1001/code/functions/')\n",
    "from modeling_functions import *\n",
    "data_dir = '/Users/jackepstein/Documents/GitHub/wildfires-1001/data'\n",
    "code_dir = '/Users/jackepstein/Documents/GitHub/wildfires-1001/code'\n",
    "model_dir = '/Users/jackepstein/Documents/GitHub/wildfires-1001/models'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report, multilabel_confusion_matrix\n",
    "from sklearn.metrics import auc, roc_curve, roc_auc_score\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC, LinearSVC\n",
    "from sklearn.preprocessing import scale, label_binarize\n",
    "from sklearn.multiclass import OneVsRestClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pull in main data frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pull in the target data frame and weather dictionary \n",
    "#make sure to change the pkl file name if needed\n",
    "target_dict = {}\n",
    "target_df = gpd.GeoDataFrame()\n",
    "for i in np.arange(1, 3):\n",
    "    target_dict[i] = pd.read_pickle(os.path.join(data_dir, f'clean_data/target_df_final_1123_newtargets_{i}.pkl')) \n",
    "    target_df = target_df.append(target_dict[i])\n",
    "\n",
    "\n",
    "weather_dict_path = os.path.join(data_dir, 'clean_data/ERA_weather-data/ERA_rename_dictionary.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load the naming dictionary\n",
    "with open(weather_dict_path, 'rb') as handle:\n",
    "    rename_dict = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#rename the columns based on this dictionary\n",
    "target_df.rename(columns = rename_dict, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create lists of columns to drop and what our targets are\n",
    "non_mod_cols = ['GRID_ID','month_id','MONTH','COUNTYFP','NAME','GRID_AREA','COUNTY_ARE','COUNTY_AREA',\n",
    "                'geometry', 'Fire_area','Index','index']\n",
    "bad_features = ['hist_p_time_1m', 'total_fire_days', 'hist_p_time_1y','month_id_old']\n",
    "Y_cols = ['Y_bin', 'Y_fire_count', 'Y_fire_area_prop', 'Y_fire_class_size','Y_bin_new_fire_month',\n",
    "          'Y_max_new_fire_size_month','Y_count_new_fires_month']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert floats from 64 to 32 for model\n",
    "for col in target_df.columns:\n",
    "    if target_df[col].dtypes == 'float64':\n",
    "        target_df[col] = target_df[col].astype(np.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pull in Models and Feature Lists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pull in models\n",
    "\n",
    "#list of models\n",
    "model_list = ['LR_15PCA_1990_2015.pkl', 'LR_30entropy_1990_2015.pkl', 'linSVC_25PCA_1990_2015.pkl', \n",
    "              'LR_15PCA_1990_2005.pkl', 'LR_20gini_1990_2005.pkl', 'linSVC_15PCA_1990_2005.pkl', \n",
    "              'linSVC_30gini_1990_2005.pkl', 'linSVC_35entropy_1990_2015.pkl']\n",
    "\n",
    "\n",
    "#get all paths for loading\n",
    "model_path_list = []\n",
    "for m in model_list:\n",
    "    mod_path = os.path.join(model_dir, m)\n",
    "    model_path_list.append(mod_path)\n",
    "\n",
    "#load the models into a dictionary with the file as the key and the model as the value\n",
    "models = {}\n",
    "for m in range(len(model_list)):\n",
    "    with open(model_path_list[m], 'rb') as handle:\n",
    "        models[model_list[m]] = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pull in feature lists\n",
    "\n",
    "#w ill need 30e\n",
    "feat_list = ['RF_entropy_top30_features.pkl', 'RF_gini_top20_features_1990_2005.pkl', \n",
    "             'RF_gini_top30_features_1990_2005.pkl', 'RF_entropy_top35_features.pkl']\n",
    "\n",
    "#get paths for loading\n",
    "feat_path_list = []\n",
    "for f in feat_list:\n",
    "    feat_path = os.path.join(model_dir,'feature_lists',f)\n",
    "    feat_path_list.append(feat_path)\n",
    "\n",
    "features = {}\n",
    "for f in range(len(feat_list)):\n",
    "    with open(feat_path_list[f], 'rb') as handle:\n",
    "        features[feat_list[f]] = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Split training and testing data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This is the split where using the 1990-2015 data all as training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#generate training data set\n",
    "#pre 2016\n",
    "train_data = target_df[target_df['YEAR']<2016]\n",
    "X_train = train_data.drop('YEAR', axis = 1)\n",
    "#drop columns not used for modeling - dont drop Ys here\n",
    "for y in Y_cols + non_mod_cols + bad_features:\n",
    "    try:\n",
    "        X_train.drop(y, inplace = True, axis =1)\n",
    "    except:\n",
    "        pass\n",
    "#set up target variable\n",
    "Y_train_cl = train_data[['Y_bin_new_fire_month']]\n",
    "Y_train_cl_size = train_data[['Y_max_new_fire_size_month']]\n",
    "Y_train_cl_arr = Y_train_cl.to_numpy().ravel()\n",
    "Y_train_size_arr = Y_train_cl_size.to_numpy().ravel()\n",
    "\n",
    "#generate testing data set - same logic as above\n",
    "test_data = target_df[target_df['YEAR']>=2016]\n",
    "X_test = test_data.drop('YEAR', axis = 1)\n",
    "for y in Y_cols + non_mod_cols + bad_features:\n",
    "    try:\n",
    "        X_test.drop(y, inplace = True, axis =1)\n",
    "    except:\n",
    "        pass\n",
    "Y_test_cl = test_data[['Y_bin_new_fire_month']]\n",
    "Y_test_cl_size = test_data[['Y_max_new_fire_size_month']]\n",
    "Y_test_cl_arr = Y_test_cl.to_numpy().ravel()\n",
    "Y_test_size_arr = Y_test_cl_size.to_numpy().ravel()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This is the split where using the 1990-2005 on initial test, 2006-2015 on 2nd stage and 2016-2019 on final testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#generate phase1 data set\n",
    "#pre 2006\n",
    "phase1_data = target_df[target_df['YEAR']<2006]\n",
    "X_phase1 = phase1_data.drop('YEAR', axis = 1)\n",
    "#drop columns not used for modeling - dont drop Ys here\n",
    "for y in Y_cols + non_mod_cols + bad_features:\n",
    "    try:\n",
    "        X_phase1.drop(y, inplace = True, axis =1)\n",
    "    except:\n",
    "        pass\n",
    "#set up target variable\n",
    "Y_ph1_cl = phase1_data[['Y_bin_new_fire_month']]\n",
    "Y_ph1_cl_size = phase1_data[['Y_max_new_fire_size_month']]\n",
    "Y_ph1_cl_arr = Y_ph1_cl.to_numpy().ravel()\n",
    "Y_ph1_cl_size_arr = Y_ph1_cl_size.to_numpy().ravel()\n",
    "\n",
    "#generate phase2 data set - same logic as above\n",
    "phase2_data = target_df[(target_df['YEAR']>=2006)&(target_df['YEAR']<2016)]\n",
    "X_phase2 = phase2_data.drop('YEAR', axis = 1)\n",
    "for y in Y_cols + non_mod_cols + bad_features:\n",
    "    try:\n",
    "        X_phase2.drop(y, inplace = True, axis =1)\n",
    "    except:\n",
    "        pass\n",
    "Y_ph2_cl = phase2_data[['Y_bin_new_fire_month']]\n",
    "Y_ph2_cl_size = phase2_data[['Y_max_new_fire_size_month']]\n",
    "Y_ph2_cl_arr = Y_ph2_cl.to_numpy().ravel()\n",
    "Y_ph2_cl_size_arr = Y_ph2_cl_size.to_numpy().ravel()\n",
    "\n",
    "#generate phase3 (test) data set\n",
    "phase3_data = target_df[target_df['YEAR']>=2016]\n",
    "X_phase3 = phase3_data.drop('YEAR', axis = 1)\n",
    "for y in Y_cols + non_mod_cols + bad_features:\n",
    "    try:\n",
    "        X_phase3.drop(y, inplace = True, axis =1)\n",
    "    except:\n",
    "        pass\n",
    "Y_ph3_cl = phase3_data[['Y_bin_new_fire_month']]\n",
    "Y_ph3_cl_size = phase3_data[['Y_max_new_fire_size_month']]\n",
    "Y_ph3_cl_arr = Y_ph3_cl.to_numpy().ravel()\n",
    "Y_ph3_cl_size_arr = Y_ph3_cl_size.to_numpy().ravel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#scale all data sets\n",
    "X_train_scaled = pd.DataFrame(scale(X_train), columns = X_train.columns, index=X_train.index)\n",
    "X_test_scaled = pd.DataFrame(scale(X_test), columns = X_test.columns, index=X_test.index)\n",
    "X_phase1_scaled = pd.DataFrame(scale(X_phase1), columns = X_test.columns, index=X_phase1.index)\n",
    "X_phase2_scaled = pd.DataFrame(scale(X_phase2), columns = X_test.columns, index=X_phase2.index)\n",
    "X_phase3_scaled = pd.DataFrame(scale(X_phase3), columns = X_test.columns, index=X_phase3.index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test 1: Taking LR model, 30entropy features, 1990-2015"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1A. Take all instances in the training set that are predicted binary positive and test size classification on the testing set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#1. slim down features\n",
    "X_tr_sc_30featentr = X_train_scaled[features['RF_entropy_top30_features.pkl']]\n",
    "X_test_sc_30featentr = X_test_scaled[features['RF_entropy_top30_features.pkl']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#2. run model on train set to get predictions\n",
    "y_preds_test1 = models['LR_30entropy_1990_2015.pkl'].predict(X_tr_sc_30featentr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#3. store predictions and filter\n",
    "\n",
    "#append the these predictions to the dataframe\n",
    "preds = pd.DataFrame(y_preds_test1, columns=['preds'], index=X_tr_sc_30featentr.index)\n",
    "X_tr_sc_30featentr_preds = X_tr_sc_30featentr.merge(preds, how='outer', left_index=True, right_index=True)\n",
    "\n",
    "\n",
    "#rejoin with the y-size column\n",
    "X_tr_sc_30featentr_ysize = X_tr_sc_30featentr_preds.merge(Y_train_cl_size, how='outer', left_index=True, right_index=True)\n",
    "\n",
    "\n",
    "#filter for the positive predicted instances\n",
    "X_tr_sc_30featentr_cut = X_tr_sc_30featentr_ysize.loc[X_tr_sc_30featentr_ysize['preds']==1]\n",
    "\n",
    "\n",
    "#drop preds and class_size columns, while saving class size as a new training y\n",
    "Y_train_cl_size_cut1 = X_tr_sc_30featentr_cut['Y_max_new_fire_size_month']\n",
    "X_tr_sc_30featentr_ready = X_tr_sc_30featentr_cut.drop(columns=['preds','Y_max_new_fire_size_month'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#regularization hyperparam options\n",
    "cs = [10**i for i in range(-4, 2)] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#4. run LR on all Cs and store all confusion matrices and AUCs\n",
    "conf_mats_lr_t1 = {}\n",
    "aucs_lr_t1 = {}\n",
    "for c in cs:\n",
    "    lr = LogisticRegression(C=c, max_iter=1500, class_weight = 'balanced').fit(X_tr_sc_30featentr_ready, \n",
    "                                                                               Y_train_cl_size_cut1.to_numpy().ravel())\n",
    "\n",
    "    cm = confusion_matrix(Y_test_size_arr, lr.predict(X_test_sc_30featentr), normalize='true')\n",
    "    \n",
    "    #store aucs -- will aggregate after taking an auc for each class -- simple mean\n",
    "    y = label_binarize(Y_test_size_arr, classes=[0,1,2,3])\n",
    "    n_classes = y.shape[1]\n",
    "    aucs_sub = np.zeros(n_classes)\n",
    "    for n in range(n_classes):\n",
    "        fpr, tpr, thresholds = roc_curve(y[:,n], lr.predict_proba(X_test_sc_30featentr)[:,n])\n",
    "        aucs_sub = auc(fpr,tpr)\n",
    "    \n",
    "    aucs_lr_t1[c] = np.mean(aucs_sub)\n",
    "    conf_mats_lr_t1[c] = cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0.0001: 0.796473482371128,\n",
       " 0.001: 0.8314413758249598,\n",
       " 0.01: 0.8428852785540163,\n",
       " 0.1: 0.8459346572328914,\n",
       " 1: 0.846694214876033,\n",
       " 10: 0.8468562340210476}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "aucs_lr_t1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#5. run SVM on all Cs and store all confusion matrices and AUCs\n",
    "conf_mats_svm_t1 = {}\n",
    "aucs_svm_t1 = {}\n",
    "for c in cs:\n",
    "    svm = LinearSVC(C=c, class_weight = 'balanced', dual=False).fit(X_tr_sc_30featentr_ready, \n",
    "                                                                               Y_train_cl_size_cut1.to_numpy().ravel())\n",
    "\n",
    "    cm = confusion_matrix(Y_test_size_arr, svm.predict(X_test_sc_30featentr), normalize='true')\n",
    "    \n",
    "    #store aucs -- will aggregate after taking an auc for each class -- simple mean\n",
    "    y = label_binarize(Y_test_size_arr, classes=[0,1,2,3])\n",
    "    n_classes = y.shape[1]\n",
    "    aucs_sub = np.zeros(n_classes)\n",
    "    for n in range(n_classes):\n",
    "        fpr, tpr, thresholds = roc_curve(y[:,n], svm.decision_function(X_test_sc_30featentr)[:,n])\n",
    "        aucs_sub = auc(fpr,tpr)\n",
    "    \n",
    "    aucs_svm_t1[c] = np.mean(aucs_sub)\n",
    "    conf_mats_svm_t1[c] = cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0.0001: 0.34535569891194484,\n",
       " 0.001: 0.7812971936500387,\n",
       " 0.01: 0.8308326892205243,\n",
       " 0.1: 0.8658377430287175,\n",
       " 1: 0.8685274094773768,\n",
       " 10: 0.868768208573637}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "aucs_svm_t1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1B. Same test data set and same features, only predicting on the actual positive instances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#filter for only the postitve instances (not positive preds)\n",
    "X_tr_sc_30featentr_cut2 = X_tr_sc_30featentr_ysize.loc[X_tr_sc_30featentr_ysize['Y_max_new_fire_size_month']>0]\n",
    "\n",
    "\n",
    "#drop preds and class_size columns, while saving class size as a new training y\n",
    "Y_train_cl_size_cut2 = X_tr_sc_30featentr_cut2['Y_max_new_fire_size_month']\n",
    "X_tr_sc_30featentr_ready2 = X_tr_sc_30featentr_cut2.drop(columns=['preds','Y_max_new_fire_size_month'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#run LR on all Cs and store all confusion matrices and AUCs\n",
    "#still need to store AUCs - HAVING ISSUES BECAUSE WE DONT HAVE ANY 0'S (MAYBE DO N=3)\n",
    "conf_mats_lr_t1b = {}\n",
    "aucs_lr_t1b = {}\n",
    "for c in cs:\n",
    "    lr = LogisticRegression(C=c, max_iter=1500, class_weight = 'balanced').fit(X_tr_sc_30featentr_ready2, \n",
    "                                                                               Y_train_cl_size_cut2.to_numpy().ravel())\n",
    "\n",
    "    cm = confusion_matrix(Y_test_size_arr, lr.predict(X_test_sc_30featentr), normalize='true')\n",
    "    \n",
    "    #store aucs -- will aggregate after taking an auc for each class -- simple mean\n",
    "    y = label_binarize(Y_test_size_arr, classes=[0,1,2,3])\n",
    "    n_classes = y.shape[1]\n",
    "    aucs_sub = np.zeros(n_classes)\n",
    "    for n in range(n_classes-1):\n",
    "        fpr, tpr, thresholds = roc_curve(y[:,n], lr.predict_proba(X_test_sc_30featentr)[:,n])\n",
    "        aucs_sub = auc(fpr,tpr)\n",
    "    \n",
    "    aucs_lr_t1b[c] = np.mean(aucs_sub)\n",
    "    conf_mats_lr_t1b[c] = cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0.0001: 0.736455353049966,\n",
       " 0.001: 0.7168862526279184,\n",
       " 0.01: 0.7091852662693833,\n",
       " 0.1: 0.7122557814204195,\n",
       " 1: 0.7129216524666867,\n",
       " 10: 0.7129690735501004}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "aucs_lr_t1b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#run SVM on all Cs and store all confusion matrices and AUCs\n",
    "conf_mats_svm_t1b = {}\n",
    "aucs_svm_t1b = {}\n",
    "for c in cs:\n",
    "    svm = LinearSVC(C=c, class_weight = 'balanced', dual=False).fit(X_tr_sc_30featentr_ready2, \n",
    "                                                                               Y_train_cl_size_cut2.to_numpy().ravel())\n",
    "\n",
    "    cm = confusion_matrix(Y_test_size_arr, svm.predict(X_test_sc_30featentr), normalize='true')\n",
    "    \n",
    "    #store aucs -- will aggregate after taking an auc for each class -- simple mean\n",
    "    y = label_binarize(Y_test_size_arr, classes=[0,1,2,3])\n",
    "    n_classes = y.shape[1]\n",
    "    aucs_sub = np.zeros(n_classes)\n",
    "    for n in range(n_classes-1):\n",
    "        fpr, tpr, thresholds = roc_curve(y[:,n], svm.decision_function(X_test_sc_30featentr)[:,n])\n",
    "        aucs_sub = auc(fpr,tpr)\n",
    "    \n",
    "    aucs_svm_t1b[c] = np.mean(aucs_sub)\n",
    "    conf_mats_svm_t1b[c] = cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0.0001: 0.271604255251885,\n",
       " 0.001: 0.5739205775887959,\n",
       " 0.01: 0.695472669648926,\n",
       " 0.1: 0.7131666613976574,\n",
       " 1: 0.7149736022635664,\n",
       " 10: 0.7151563710225565}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "aucs_svm_t1b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test 2: Taking LR model, 20gini features, train on 2006-2015"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2A. Use predictive positives in 2006-2015 as training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#use the 'LR_20gini_1990_2005.pkl' model\n",
    "#use the 'RF_gini_top20_features_1990_2005.pkl' feature list\n",
    "#key -- use 1990-2005 to get predictions of positive instances in 2006-2015 using imported model\n",
    "#then use these positive predictions to train LR/SVM models for class size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#1. slim down features\n",
    "X_ph1_sc_20gi = X_phase1_scaled[features['RF_gini_top20_features_1990_2005.pkl']]\n",
    "X_ph2_sc_20gi = X_phase2_scaled[features['RF_gini_top20_features_1990_2005.pkl']]\n",
    "X_ph3_sc_20gi = X_phase3_scaled[features['RF_gini_top20_features_1990_2005.pkl']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#2. run model on train set to get predictions\n",
    "y_preds_test2 = models['LR_20gini_1990_2005.pkl'].predict(X_ph2_sc_20gi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "#3. store predictions and filter\n",
    "\n",
    "#append the these predictions to the dataframe\n",
    "preds2 = pd.DataFrame(y_preds_test2, columns=['preds'], index=X_ph2_sc_20gi.index)\n",
    "X_ph2_sc_20gi_preds = X_ph2_sc_20gi.merge(preds2, how='outer', left_index=True, right_index=True)\n",
    "\n",
    "\n",
    "#rejoin with the y-size column\n",
    "X_ph2_sc_20gi_ysize = X_ph2_sc_20gi_preds.merge(Y_ph2_cl_size, how='outer', left_index=True, right_index=True)\n",
    "\n",
    "\n",
    "#filter for the positive predicted instances\n",
    "X_ph2_sc_20gi_cut = X_ph2_sc_20gi_ysize.loc[X_ph2_sc_20gi_ysize['preds']==1]\n",
    "\n",
    "\n",
    "#drop preds and class_size columns, while saving class size as a new training y\n",
    "Y_ph2_cl_size_cut1 = X_ph2_sc_20gi_cut['Y_max_new_fire_size_month']\n",
    "X_ph2_sc_20gi_ready = X_ph2_sc_20gi_cut.drop(columns=['preds','Y_max_new_fire_size_month'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "#4. run LR on all Cs and store all confusion matrices and AUCs\n",
    "conf_mats_lr_t2 = {}\n",
    "aucs_lr_t2 = {}\n",
    "for c in cs:\n",
    "    lr = LogisticRegression(C=c, max_iter=1500, class_weight = 'balanced').fit(X_ph2_sc_20gi_ready, \n",
    "                                                                               Y_ph2_cl_size_cut1.to_numpy().ravel())\n",
    "\n",
    "    cm = confusion_matrix(Y_ph3_cl_size_arr, lr.predict(X_ph3_sc_20gi), normalize='true')\n",
    "    \n",
    "    #store aucs -- will aggregate after taking an auc for each class -- simple mean\n",
    "    y = label_binarize(Y_ph3_cl_size_arr, classes=[0,1,2,3])\n",
    "    n_classes = y.shape[1]\n",
    "    aucs_sub = np.zeros(n_classes)\n",
    "    for n in range(n_classes):\n",
    "        fpr, tpr, thresholds = roc_curve(y[:,n], lr.predict_proba(X_ph3_sc_20gi)[:,n])\n",
    "        aucs_sub = auc(fpr,tpr)\n",
    "    \n",
    "    aucs_lr_t2[c] = np.mean(aucs_sub)\n",
    "    conf_mats_lr_t2[c] = cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0.0001: 0.7056238480290148,\n",
       " 0.001: 0.7542489149176527,\n",
       " 0.01: 0.8331031868719903,\n",
       " 0.1: 0.8477993638147333,\n",
       " 1: 0.8496573815327902,\n",
       " 10: 0.8498513585825554}"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "aucs_lr_t2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "#5. run SVM on all Cs and store all confusion matrices and AUCs\n",
    "conf_mats_svm_t2 = {}\n",
    "aucs_svm_t2 = {}\n",
    "for c in cs:\n",
    "    svm = LinearSVC(C=c, class_weight = 'balanced', dual=False).fit(X_ph2_sc_20gi_ready, \n",
    "                                                                               Y_ph2_cl_size_cut1.to_numpy().ravel())\n",
    "\n",
    "    cm = confusion_matrix(Y_ph3_cl_size_arr, svm.predict(X_ph3_sc_20gi), normalize='true')\n",
    "    \n",
    "    #store aucs -- will aggregate after taking an auc for each class -- simple mean\n",
    "    y = label_binarize(Y_ph3_cl_size_arr, classes=[0,1,2,3])\n",
    "    n_classes = y.shape[1]\n",
    "    aucs_sub = np.zeros(n_classes)\n",
    "    for n in range(n_classes):\n",
    "        fpr, tpr, thresholds = roc_curve(y[:,n], svm.decision_function(X_ph3_sc_20gi)[:,n])\n",
    "        aucs_sub = auc(fpr,tpr)\n",
    "    \n",
    "    aucs_svm_t2[c] = np.mean(aucs_sub)\n",
    "    conf_mats_svm_t2[c] = cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0.0001: 0.19141670134966407,\n",
       " 0.001: 0.4899481241453118,\n",
       " 0.01: 0.8541106783994291,\n",
       " 0.1: 0.8672884832629764,\n",
       " 1: 0.8682397883346217,\n",
       " 10: 0.8683445805339199}"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "aucs_svm_t2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2B. Same test data set and same features, only predicting on the actual positive instances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "#filter for only the postitve instances in the phase 2 set(not positive preds)\n",
    "X_ph2_sc_20gi_cut2 = X_ph2_sc_20gi_ysize.loc[X_ph2_sc_20gi_ysize['Y_max_new_fire_size_month']>0]\n",
    "\n",
    "\n",
    "#drop preds and class_size columns, while saving class size as a new training y\n",
    "Y_ph2_cl_size_cut2 = X_ph2_sc_20gi_cut2['Y_max_new_fire_size_month']\n",
    "X_ph2_sc_20gi_ready2 = X_ph2_sc_20gi_cut2.drop(columns=['preds','Y_max_new_fire_size_month'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "#run LR on all Cs and store all confusion matrices and AUCs\n",
    "conf_mats_lr_t2b = {}\n",
    "aucs_lr_t2b = {}\n",
    "for c in cs:\n",
    "    lr = LogisticRegression(C=c, max_iter=1500, class_weight = 'balanced').fit(X_ph2_sc_20gi_ready2, \n",
    "                                                                               Y_ph2_cl_size_cut2.to_numpy().ravel())\n",
    "\n",
    "    cm = confusion_matrix(Y_ph3_cl_size_arr, lr.predict(X_ph3_sc_20gi), normalize='true')\n",
    "    \n",
    "    #store aucs -- will aggregate after taking an auc for each class -- simple mean\n",
    "    y = label_binarize(Y_ph3_cl_size_arr, classes=[0,1,2,3])\n",
    "    n_classes = y.shape[1]\n",
    "    aucs_sub = np.zeros(n_classes)\n",
    "    for n in range(n_classes-1):\n",
    "        fpr, tpr, thresholds = roc_curve(y[:,n], lr.predict_proba(X_ph3_sc_20gi)[:,n])\n",
    "        aucs_sub = auc(fpr,tpr)\n",
    "    \n",
    "    aucs_lr_t2b[c] = np.mean(aucs_sub)\n",
    "    conf_mats_lr_t2b[c] = cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0.0001: 0.716391295069788,\n",
       " 0.001: 0.7132585397467713,\n",
       " 0.01: 0.7113715758026018,\n",
       " 0.1: 0.69361534388189,\n",
       " 1: 0.6854233517221757,\n",
       " 10: 0.6842220176090289}"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "aucs_lr_t2b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "#run SVM on all Cs and store all confusion matrices and AUCs\n",
    "conf_mats_svm_t2b = {}\n",
    "aucs_svm_t2b = {}\n",
    "for c in cs:\n",
    "    svm = LinearSVC(C=c, class_weight = 'balanced', dual=False).fit(X_ph2_sc_20gi_ready2, \n",
    "                                                                               Y_ph2_cl_size_cut2.to_numpy().ravel())\n",
    "\n",
    "    cm = confusion_matrix(Y_ph3_cl_size_arr, svm.predict(X_ph3_sc_20gi), normalize='true')\n",
    "    \n",
    "    #store aucs -- will aggregate after taking an auc for each class -- simple mean\n",
    "    y = label_binarize(Y_ph3_cl_size_arr, classes=[0,1,2,3])\n",
    "    n_classes = y.shape[1]\n",
    "    aucs_sub = np.zeros(n_classes)\n",
    "    for n in range(n_classes-1):\n",
    "        fpr, tpr, thresholds = roc_curve(y[:,n], svm.decision_function(X_ph3_sc_20gi)[:,n])\n",
    "        aucs_sub = auc(fpr,tpr)\n",
    "    \n",
    "    aucs_svm_t2b[c] = np.mean(aucs_sub)\n",
    "    conf_mats_svm_t2b[c] = cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0.0001: 0.24192557260958222,\n",
       " 0.001: 0.4725817223337496,\n",
       " 0.01: 0.6686797575201935,\n",
       " 0.1: 0.6892585318432574,\n",
       " 1: 0.6908323190490492,\n",
       " 10: 0.690975570238528}"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "aucs_svm_t2b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test 3: Taking SVM model, 30entropy features, 1990-2015"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use 'linSVC_35entropy_1990_2015.pkl' model\n",
    "# use 'RF_entropy_top35_features.pkl' features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3A. Take all instances in the training set that are predicted binary positive and test size classification on the testing set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "#1. slim down features\n",
    "X_tr_sc_35featentr = X_train_scaled[features['RF_entropy_top35_features.pkl']]\n",
    "X_test_sc_35featentr = X_test_scaled[features['RF_entropy_top35_features.pkl']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "#2. run model on train set to get predictions\n",
    "y_preds_test3 = models['linSVC_35entropy_1990_2015.pkl'].predict(X_tr_sc_35featentr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "#3. store predictions and filter\n",
    "\n",
    "#append the these predictions to the dataframe\n",
    "preds3 = pd.DataFrame(y_preds_test3, columns=['preds'], index=X_tr_sc_35featentr.index)\n",
    "X_tr_sc_35featentr_preds = X_tr_sc_35featentr.merge(preds3, how='outer', left_index=True, right_index=True)\n",
    "\n",
    "\n",
    "#rejoin with the y-size column\n",
    "X_tr_sc_35featentr_ysize = X_tr_sc_35featentr_preds.merge(Y_train_cl_size, how='outer', left_index=True, right_index=True)\n",
    "\n",
    "\n",
    "#filter for the positive predicted instances\n",
    "X_tr_sc_35featentr_cut = X_tr_sc_35featentr_ysize.loc[X_tr_sc_35featentr_ysize['preds']==1]\n",
    "\n",
    "\n",
    "#drop preds and class_size columns, while saving class size as a new training y\n",
    "Y_train_cl_size_cut3 = X_tr_sc_35featentr_cut['Y_max_new_fire_size_month']\n",
    "X_tr_sc_35featentr_ready = X_tr_sc_35featentr_cut.drop(columns=['preds','Y_max_new_fire_size_month'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "#4. run LR on all Cs and store all confusion matrices and AUCs\n",
    "conf_mats_lr_t3 = {}\n",
    "aucs_lr_t3 = {}\n",
    "for c in cs:\n",
    "    lr = LogisticRegression(C=c, max_iter=1500, class_weight = 'balanced').fit(X_tr_sc_35featentr_ready, \n",
    "                                                                               Y_train_cl_size_cut3.to_numpy().ravel())\n",
    "\n",
    "    cm = confusion_matrix(Y_test_size_arr, lr.predict(X_test_sc_35featentr), normalize='true')\n",
    "    \n",
    "    #store aucs -- will aggregate after taking an auc for each class -- simple mean\n",
    "    y = label_binarize(Y_test_size_arr, classes=[0,1,2,3])\n",
    "    n_classes = y.shape[1]\n",
    "    aucs_sub = np.zeros(n_classes)\n",
    "    for n in range(n_classes):\n",
    "        fpr, tpr, thresholds = roc_curve(y[:,n], lr.predict_proba(X_test_sc_35featentr)[:,n])\n",
    "        aucs_sub = auc(fpr,tpr)\n",
    "    \n",
    "    aucs_lr_t3[c] = np.mean(aucs_sub)\n",
    "    conf_mats_lr_t3[c] = cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0.0001: 0.7810006540222367,\n",
       " 0.001: 0.8390874903383079,\n",
       " 0.01: 0.8555978357809619,\n",
       " 0.1: 0.8600399845412925,\n",
       " 1: 0.8611064867114572,\n",
       " 10: 0.8612142517391046}"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "aucs_lr_t3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "#5. run SVM on all Cs and store all confusion matrices and AUCs\n",
    "conf_mats_svm_t3 = {}\n",
    "aucs_svm_t3 = {}\n",
    "for c in cs:\n",
    "    svm = LinearSVC(C=c, class_weight = 'balanced', dual=False).fit(X_tr_sc_35featentr_ready, \n",
    "                                                                               Y_train_cl_size_cut3.to_numpy().ravel())\n",
    "\n",
    "    cm = confusion_matrix(Y_test_size_arr, svm.predict(X_test_sc_35featentr), normalize='true')\n",
    "    \n",
    "    #store aucs -- will aggregate after taking an auc for each class -- simple mean\n",
    "    y = label_binarize(Y_test_size_arr, classes=[0,1,2,3])\n",
    "    n_classes = y.shape[1]\n",
    "    aucs_sub = np.zeros(n_classes)\n",
    "    for n in range(n_classes):\n",
    "        fpr, tpr, thresholds = roc_curve(y[:,n], svm.decision_function(X_test_sc_35featentr)[:,n])\n",
    "        aucs_sub = auc(fpr,tpr)\n",
    "    \n",
    "    aucs_svm_t3[c] = np.mean(aucs_sub)\n",
    "    conf_mats_svm_t3[c] = cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0.0001: 0.4024258279326952,\n",
       " 0.001: 0.7917191866341639,\n",
       " 0.01: 0.8618534098341162,\n",
       " 0.1: 0.8765889767524823,\n",
       " 1: 0.8773195493192224,\n",
       " 10: 0.877375289850764}"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "aucs_svm_t3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3B. Same test data set and same features, only predicting on the actual positive instances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "#filter for only the postitve instances (not positive preds)\n",
    "X_tr_sc_35featentr_cut3 = X_tr_sc_35featentr_ysize.loc[X_tr_sc_35featentr_ysize['Y_max_new_fire_size_month']>0]\n",
    "\n",
    "\n",
    "#drop preds and class_size columns, while saving class size as a new training y\n",
    "Y_train_cl_size_cut3b = X_tr_sc_35featentr_cut3['Y_max_new_fire_size_month']\n",
    "X_tr_sc_35featentr_ready2 = X_tr_sc_35featentr_cut3.drop(columns=['preds','Y_max_new_fire_size_month'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "#run LR on all Cs and store all confusion matrices and AUCs\n",
    "conf_mats_lr_t3b = {}\n",
    "aucs_lr_t3b = {}\n",
    "for c in cs:\n",
    "    lr = LogisticRegression(C=c, max_iter=1500, class_weight = 'balanced').fit(X_tr_sc_35featentr_ready2, \n",
    "                                                                               Y_train_cl_size_cut3b.to_numpy().ravel())\n",
    "\n",
    "    cm = confusion_matrix(Y_test_size_arr, lr.predict(X_test_sc_35featentr), normalize='true')\n",
    "    \n",
    "    #store aucs -- will aggregate after taking an auc for each class -- simple mean\n",
    "    y = label_binarize(Y_test_size_arr, classes=[0,1,2,3])\n",
    "    n_classes = y.shape[1]\n",
    "    aucs_sub = np.zeros(n_classes)\n",
    "    for n in range(n_classes-1):\n",
    "        fpr, tpr, thresholds = roc_curve(y[:,n], lr.predict_proba(X_test_sc_35featentr)[:,n])\n",
    "        aucs_sub = auc(fpr,tpr)\n",
    "    \n",
    "    aucs_lr_t3b[c] = np.mean(aucs_sub)\n",
    "    conf_mats_lr_t3b[c] = cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0.0001: 0.682612664590677,\n",
       " 0.001: 0.6941231446501115,\n",
       " 0.01: 0.7092623255299307,\n",
       " 0.1: 0.7170433349667262,\n",
       " 1: 0.718073755591736,\n",
       " 10: 0.7181774892117035}"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "aucs_lr_t3b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "#run SVM on all Cs and store all confusion matrices and AUCs\n",
    "conf_mats_svm_t3b = {}\n",
    "aucs_svm_t3b = {}\n",
    "for c in cs:\n",
    "    svm = LinearSVC(C=c, class_weight = 'balanced', dual=False).fit(X_tr_sc_35featentr_ready2, \n",
    "                                                                               Y_train_cl_size_cut3b.to_numpy().ravel())\n",
    "\n",
    "    cm = confusion_matrix(Y_test_size_arr, svm.predict(X_test_sc_35featentr), normalize='true')\n",
    "    \n",
    "    #store aucs -- will aggregate after taking an auc for each class -- simple mean\n",
    "    y = label_binarize(Y_test_size_arr, classes=[0,1,2,3])\n",
    "    n_classes = y.shape[1]\n",
    "    aucs_sub = np.zeros(n_classes)\n",
    "    for n in range(n_classes-1):\n",
    "        fpr, tpr, thresholds = roc_curve(y[:,n], svm.decision_function(X_test_sc_35featentr)[:,n])\n",
    "        aucs_sub = auc(fpr,tpr)\n",
    "    \n",
    "    aucs_svm_t3b[c] = np.mean(aucs_sub)\n",
    "    conf_mats_svm_t3b[c] = cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0.0001: 0.28688471144270744,\n",
       " 0.001: 0.5606930591340911,\n",
       " 0.01: 0.697494981268672,\n",
       " 0.1: 0.7172033811232474,\n",
       " 1: 0.7191565369963486,\n",
       " 10: 0.7193659801147589}"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "aucs_svm_t3b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test 4: Taking SVM model, 30entropy features, 2005-2015 as testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['RF_entropy_top30_features.pkl', 'RF_gini_top20_features_1990_2005.pkl', 'RF_gini_top30_features_1990_2005.pkl', 'RF_entropy_top35_features.pkl'])"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# use 'linSVC_30gini_1990_2005.pkl' model\n",
    "# use 'RF_gini_top30_features_1990_2005.pkl' features\n",
    "features.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "#1. slim down features\n",
    "X_ph1_sc_30gi = X_phase1_scaled[features['RF_gini_top30_features_1990_2005.pkl']]\n",
    "X_ph2_sc_30gi = X_phase2_scaled[features['RF_gini_top30_features_1990_2005.pkl']]\n",
    "X_ph3_sc_30gi = X_phase3_scaled[features['RF_gini_top30_features_1990_2005.pkl']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "#2. run model on train set to get predictions\n",
    "y_preds_test4 = models['linSVC_30gini_1990_2005.pkl'].predict(X_ph2_sc_30gi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "#3. store predictions and filter\n",
    "\n",
    "#append the these predictions to the dataframe\n",
    "preds4 = pd.DataFrame(y_preds_test4, columns=['preds'], index=X_ph2_sc_30gi.index)\n",
    "X_ph2_sc_30gi_preds = X_ph2_sc_30gi.merge(preds4, how='outer', left_index=True, right_index=True)\n",
    "\n",
    "\n",
    "#rejoin with the y-size column\n",
    "X_ph2_sc_30gi_ysize = X_ph2_sc_30gi_preds.merge(Y_ph2_cl_size, how='outer', left_index=True, right_index=True)\n",
    "\n",
    "\n",
    "#filter for the positive predicted instances\n",
    "X_ph2_sc_30gi_cut = X_ph2_sc_30gi_ysize.loc[X_ph2_sc_30gi_ysize['preds']==1]\n",
    "\n",
    "\n",
    "#drop preds and class_size columns, while saving class size as a new training y\n",
    "Y_ph2_cl_size_cut4 = X_ph2_sc_30gi_cut['Y_max_new_fire_size_month']\n",
    "X_ph2_sc_30gi_ready = X_ph2_sc_30gi_cut.drop(columns=['preds','Y_max_new_fire_size_month'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "#4. run LR on all Cs and store all confusion matrices and AUCs\n",
    "aucs_lr_t4 = {}\n",
    "conf_mats_lr_t4 = {}\n",
    "for c in cs:\n",
    "    lr = LogisticRegression(C=c, max_iter=1500, class_weight = 'balanced').fit(X_ph2_sc_30gi_ready, \n",
    "                                                                               Y_ph2_cl_size_cut4.to_numpy().ravel())\n",
    "\n",
    "    cm = confusion_matrix(Y_ph3_cl_size_arr, lr.predict(X_ph3_sc_30gi), normalize='true')\n",
    "    \n",
    "    #store aucs -- will aggregate after taking an auc for each class -- simple mean\n",
    "    y = label_binarize(Y_ph3_cl_size_arr, classes=[0,1,2,3])\n",
    "    n_classes = y.shape[1]\n",
    "    aucs_sub = np.zeros(n_classes)\n",
    "    for n in range(n_classes):\n",
    "        fpr, tpr, thresholds = roc_curve(y[:,n], lr.predict_proba(X_ph3_sc_30gi)[:,n])\n",
    "        aucs_sub = auc(fpr,tpr)\n",
    "    \n",
    "    aucs_lr_t4[c] = np.mean(aucs_sub)\n",
    "    conf_mats_lr_t4[c] = cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0.0001: 0.6418737737083061,\n",
       " 0.001: 0.7645557108032581,\n",
       " 0.01: 0.8519561210535704,\n",
       " 0.1: 0.8653189844818361,\n",
       " 1: 0.867006807776919,\n",
       " 10: 0.8671836910636781}"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "aucs_lr_t4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "#5. run SVM on all Cs and store all confusion matrices and AUCs\n",
    "aucs_svm_t4 = {}\n",
    "conf_mats_svm_t4 = {}\n",
    "for c in cs:\n",
    "    svm = LinearSVC(C=c, class_weight = 'balanced', dual=False).fit(X_ph2_sc_30gi_ready, \n",
    "                                                                               Y_ph2_cl_size_cut4.to_numpy().ravel())\n",
    "\n",
    "    cm = confusion_matrix(Y_ph3_cl_size_arr, svm.predict(X_ph3_sc_30gi), normalize='true')\n",
    "    \n",
    "    #store aucs -- will aggregate after taking an auc for each class -- simple mean\n",
    "    y = label_binarize(Y_ph3_cl_size_arr, classes=[0,1,2,3])\n",
    "    n_classes = y.shape[1]\n",
    "    aucs_sub = np.zeros(n_classes)\n",
    "    for n in range(n_classes):\n",
    "        fpr, tpr, thresholds = roc_curve(y[:,n], svm.decision_function(X_ph3_sc_30gi)[:,n])\n",
    "        aucs_sub = auc(fpr,tpr)\n",
    "    \n",
    "    aucs_svm_t4[c] = np.mean(aucs_sub)\n",
    "    conf_mats_svm_t4[c] = cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0.0001: 0.22068196682323563,\n",
       " 0.001: 0.6584339140258042,\n",
       " 0.01: 0.8714400380522028,\n",
       " 0.1: 0.8767316725132291,\n",
       " 1: 0.8771404364112017,\n",
       " 10: 0.8771731375230395}"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "aucs_svm_t4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4B. Same test data set and same features, only predicting on the actual positive instances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "#filter for only the postitve instances in the phase 2 set(not positive preds)\n",
    "X_ph2_sc_30gi_cut4 = X_ph2_sc_30gi_ysize.loc[X_ph2_sc_30gi_ysize['Y_max_new_fire_size_month']>0]\n",
    "\n",
    "\n",
    "#drop preds and class_size columns, while saving class size as a new training y\n",
    "Y_ph2_cl_size_cut4 = X_ph2_sc_30gi_cut4['Y_max_new_fire_size_month']\n",
    "X_ph2_sc_30gi_ready4 = X_ph2_sc_30gi_cut4.drop(columns=['preds','Y_max_new_fire_size_month'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "#run LR on all Cs and store all confusion matrices and AUCs\n",
    "conf_mats_lr_t4b = {}\n",
    "aucs_lr_t4b = {}\n",
    "for c in cs:\n",
    "    lr = LogisticRegression(C=c, max_iter=1500, class_weight = 'balanced').fit(X_ph2_sc_30gi_ready4, \n",
    "                                                                               Y_ph2_cl_size_cut4.to_numpy().ravel())\n",
    "\n",
    "    cm = confusion_matrix(Y_ph3_cl_size_arr, lr.predict(X_ph3_sc_30gi), normalize='true')\n",
    "    \n",
    "    #store aucs -- will aggregate after taking an auc for each class -- simple mean\n",
    "    y = label_binarize(Y_ph3_cl_size_arr, classes=[0,1,2,3])\n",
    "    n_classes = y.shape[1]\n",
    "    aucs_sub = np.zeros(n_classes)\n",
    "    for n in range(n_classes-1):\n",
    "        fpr, tpr, thresholds = roc_curve(y[:,n], lr.predict_proba(X_ph3_sc_30gi)[:,n])\n",
    "        aucs_sub = auc(fpr,tpr)\n",
    "    \n",
    "    aucs_lr_t4b[c] = np.mean(aucs_sub)\n",
    "    conf_mats_lr_t4b[c] = cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0.0001: 0.5664725036751339,\n",
       " 0.001: 0.6385545263424117,\n",
       " 0.01: 0.7148896274283546,\n",
       " 0.1: 0.7291159524524604,\n",
       " 1: 0.7295279231146167,\n",
       " 10: 0.7294261653731249}"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "aucs_lr_t4b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "#run SVM on all Cs and store all confusion matrices and AUCs\n",
    "conf_mats_svm_t4b = {}\n",
    "aucs_svm_t4b = {}\n",
    "for c in cs:\n",
    "    svm = LinearSVC(C=c, class_weight = 'balanced', dual=False).fit(X_ph2_sc_30gi_ready4, \n",
    "                                                                               Y_ph2_cl_size_cut4.to_numpy().ravel())\n",
    "\n",
    "    cm = confusion_matrix(Y_ph3_cl_size_arr, svm.predict(X_ph3_sc_30gi), normalize='true')\n",
    "    \n",
    "    #store aucs -- will aggregate after taking an auc for each class -- simple mean\n",
    "    y = label_binarize(Y_ph3_cl_size_arr, classes=[0,1,2,3])\n",
    "    n_classes = y.shape[1]\n",
    "    aucs_sub = np.zeros(n_classes)\n",
    "    for n in range(n_classes-1):\n",
    "        fpr, tpr, thresholds = roc_curve(y[:,n], svm.decision_function(X_ph3_sc_30gi)[:,n])\n",
    "        aucs_sub = auc(fpr,tpr)\n",
    "    \n",
    "    aucs_svm_t4b[c] = np.mean(aucs_sub)\n",
    "    conf_mats_svm_t4b[c] = cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0.0001: 0.30706831007065744,\n",
       " 0.001: 0.4863348244629562,\n",
       " 0.01: 0.6987308932551412,\n",
       " 0.1: 0.7301187107788123,\n",
       " 1: 0.7330884561275943,\n",
       " 10: 0.7333957052305455}"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "aucs_svm_t4b"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
